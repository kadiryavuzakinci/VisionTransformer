{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"BMKhlb8yyunS"},"outputs":[],"source":["!pip install torch"]},{"cell_type":"code","source":["import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import f1_score\n","import numpy as np\n","import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import matplotlib.pyplot as plt"],"metadata":{"id":"bUPFnswVy1Fg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CustomDataset(Dataset):\n","  def __init__(self, root_dir, transform=None):\n","    self.root_dir = root_dir\n","    self.transform = transform\n","    self.images = []\n","    self.labels = []\n","\n","    for class_name in os.listdir(root_dir):\n","      class_dir = os.path.join(root_dir, class_name)\n","      if os.path.isdir(class_dir):\n","        class_idx = len(self.labels)\n","        for image_name in os.listdir(class_dir):\n","          image_path = os.path.join(class_dir, image_name)\n","          self.images.append(image_path)\n","          self.labels.append(class_idx)\n","\n","  def __len__(self):\n","    return len(self.images)\n","\n","  def __getitem__(self, idx):\n","    image_path = self.images[idx]\n","    label = self.labels[idx]\n","\n","    image = Image.open(image_path).convert('RGB')\n","\n","    if self.transform:\n","      image = self.transform(image)\n","\n","    return image, label"],"metadata":{"id":"EX7JePOF4nwS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms\n","\n","transform = transforms.Compose([\n","    transforms.Resize((256, 256)),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","])\n","\n","train_dataset = CustomDataset(root_dir='path/to/your/dataset', transform=transform)\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","val_dataset = CustomDataset(root_dir='path/to/your/dataset', transform=transform)\n","val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n","\n","test_dataset = CustomDataset(root_dir='path/to/your/dataset', transform=transform)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"],"metadata":{"id":"3mfAnyyK50wk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class VisionTransformer(nn.Module):\n","    def __init__(self, image_size, patch_size, num_classes, embedding_dim=128, num_heads=4, num_layers=2):\n","        super(VisionTransformer, self).__init__()\n","\n","        # Calculate the number of patches based on the image size and patch size\n","        self.num_patches = (image_size // patch_size) ** 2\n","        self.patch_size = patch_size\n","\n","        # Input embedding layer\n","        self.input_embedding = nn.Linear(self.patch_size * self.patch_size, embedding_dim)\n","\n","        # Positional encoding\n","        self.positional_encoding = self.initialize_positional_encoding(embedding_dim, self.num_patches)\n","\n","        # Transformer encoder blocks\n","        self.encoder_blocks = nn.ModuleList([\n","            TransformerEncoderBlock(embedding_dim, num_heads) for _ in range(num_layers)\n","        ])\n","\n","        # Classification head\n","        self.classification_head = nn.Linear(embedding_dim, num_classes)\n","\n","    def initialize_positional_encoding(self, embedding_dim, num_patches):\n","        # Create positional embeddings for patches\n","        positional_encoding = torch.zeros(1, num_patches, embedding_dim)\n","        positions = torch.arange(0, num_patches, dtype=torch.float).unsqueeze(1)\n","        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-torch.log(torch.tensor(10000.0)) / embedding_dim))\n","        positional_encoding[0, :, 0::2] = torch.sin(positions * div_term)\n","        positional_encoding[0, :, 1::2] = torch.cos(positions * div_term)\n","        return nn.Parameter(positional_encoding, requires_grad=False)\n","\n","    def forward(self, x):\n","        # Reshape input images into patches\n","        patches = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n","        patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous().view(x.size(0), -1, self.patch_size * self.patch_size)\n","\n","        # Apply input embedding\n","        patches = self.input_embedding(patches)\n","\n","        # Add positional encoding\n","        patches += self.positional_encoding\n","\n","        # Transformer encoder blocks\n","        for encoder_block in self.encoder_blocks:\n","            patches = encoder_block(patches)\n","\n","        # Global average pooling\n","        output = patches.mean(1)\n","\n","        # Apply classification head\n","        output = self.classification_head(output.squeeze())  # Squeeze the tensor to remove the extra dimension\n","\n","        return output\n","\n","\n","\n","class MultiHeadAttentionLayer(nn.Module):\n","  def __init__(self, embedding_dim, num_heads, dropout=0.1):\n","    super(MultiHeadAttentionLayer, self).__init__()\n","    self.embedding_dim = embedding_dim\n","    self.num_heads = num_heads\n","\n","    # Check if embedding dimensions can be divided evenly by the number of heads\n","    assert embedding_dim % num_heads == 0, \"Embedding dimension must be divisible by the number of heads\"\n","\n","    self.head_dim = embedding_dim // num_heads\n","\n","    self.fc_q = nn.Linear(embedding_dim, embedding_dim)\n","    self.fc_k = nn.Linear(embedding_dim, embedding_dim)\n","    self.fc_v = nn.Linear(embedding_dim, embedding_dim)\n","\n","    self.dropout = nn.Dropout(dropout)\n","\n","    self.fc_out = nn.Linear(embedding_dim, embedding_dim)\n","\n","  def forward(self, query, key, value, mask=None):\n","    batch_size = query.shape[0]\n","\n","    # Apply linear transformations for query, key, and value\n","    Q = self.fc_q(query)\n","    K = self.fc_k(key)\n","    V = self.fc_v(value)\n","\n","    # Reshape the transformed Q, K, V for multi-head mechanism\n","    Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","    K = K.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","    V = V.view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n","\n","    # Compute attention scores\n","    attention_scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32))\n","\n","    if mask is not None:\n","      attention_scores = attention_scores.masked_fill(mask == 0, float(\"-1e20\"))\n","\n","    attention_weights = torch.softmax(attention_scores, dim=-1)\n","    attention_weights = self.dropout(attention_weights)\n","\n","    output = torch.matmul(attention_weights, V)\n","\n","    output = output.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.embedding_dim)\n","\n","    output = self.fc_out(output)\n","\n","    return output\n","\n","\n","class TransformerEncoderBlock(nn.Module):\n","  def __init__(self, embedding_dim, num_heads, dropout=0.1):\n","    super(TransformerEncoderBlock, self).__init__()\n","\n","    self.multihead_attention = MultiHeadAttentionLayer(embedding_dim, num_heads, dropout=dropout)\n","\n","    self.norm1 = nn.LayerNorm(embedding_dim)\n","    self.norm2 = nn.LayerNorm(embedding_dim)\n","\n","    self.feedforward = nn.Sequential(\n","        nn.Linear(embedding_dim, 4 * embedding_dim),\n","        nn.ReLU(),\n","        nn.Linear(4 * embedding_dim, embedding_dim),\n","        nn.Dropout(dropout)\n","    )\n","\n","  def forward(self, x):\n","    # Self-attention and layer normalization\n","    attention_output = self.multihead_attention(x, x, x)\n","    x = self.norm1(x + attention_output)\n","\n","    #Feedforward and layer normalization\n","    feedforward_output = self.feedforward(x)\n","    x = self.norm2(x + feedforward_output)\n","\n","    return x"],"metadata":{"id":"5GlQJgnx6kqd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(model, train_loader, criterion, optimizer, num_epochs=10, val_loader=None):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model.to(device)\n","\n","    train_losses = []\n","    val_losses = []\n","    f1_scores = []\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_train_loss = 0.0\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_train_loss += loss.item() * images.size(0)\n","\n","        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n","        train_losses.append(epoch_train_loss)\n","\n","        print(f\"Epoch [{epoch+1}/{num_epochs}] Train Loss: {epoch_train_loss:.4f}\")\n","\n","        if val_loader is not None:\n","            val_loss, f1 = evaluate_model(model, val_loader, criterion)\n","            val_losses.append(val_loss)\n","            f1_scores.append(f1)\n","\n","            print(f\"Validation Loss: {val_loss:.4f} F1 Score: {f1:.4f}\")\n","\n","    print(\"Training finished!\")\n","\n","    if val_loader is not None:\n","        plot_metrics(train_losses, val_losses, f1_scores)"],"metadata":{"id":"vfqsCTG1PfQx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def evaluate_model(model, val_loader, criterion):\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  model.eval()\n","\n","  running_val_loss = 0.0\n","  all_predictions = []\n","  all_labels = []\n","\n","  with torch.no_grad():\n","    for images, labels in val_loader:\n","      images, labels = images.to(device), labels.to(device)\n","\n","      outputs = model(images)\n","      loss = criterion(outputs, labels)\n","      running_val_loss += loss.item() * images.size(0)\n","\n","      predictions = torch.argmax(outputs, dim=1)\n","      all_predictions.extend(predictions.cpu().numpy())\n","      all_labels.extend(labels.cpu().numpy())  # Append true labels to all_labels list\n","\n","  val_loss = running_val_loss / len(val_loader.dataset)\n","  f1 = calculate_f1_score(all_labels, all_predictions)\n","\n","  return val_loss, f1"],"metadata":{"id":"N8zRmGetY_pp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def calculate_f1_score(y_true, y_pred):\n","\n","  return f1_score(y_true, y_pred, average='macro')\n","\n","\n","def plot_metrics(train_losses, val_losses, f1_scores):\n","  plt.figure(figsize=(10, 4))\n","\n","  plt.subplot(1, 2, 1)\n","  plt.plot(train_losses, label='Train Loss')\n","  plt.plot(val_losses, label='Val Loss')\n","  plt.xlabel('Epoch')\n","  plt.ylabel('Loss')\n","  plt.legend()\n","\n","  plt.subplot(1, 2, 2)\n","  plt.plot(f1_scores, label='F1 Score', color='green')\n","  plt.xlabel('Epoch')\n","  plt.ylabel('F1 Score')\n","  plt.legend()\n","\n","  plt.tight_layout()\n","  plt.show()"],"metadata":{"id":"UExYl6g1aXQi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def main():\n","\n","    image_size = 256  # Your image size\n","    patch_size = 16  # Patch size\n","    num_classes = 10\n","    model = VisionTransformer(image_size=image_size, patch_size=patch_size, num_classes=num_classes)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.Adam(model.parameters(), lr=0.001)\n","\n","    train_model(model, train_loader, criterion, optimizer, num_epochs=10, val_loader=val_loader)\n"],"metadata":{"id":"a7gxd29HP7t4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"wgajoAJCQMcP"},"execution_count":null,"outputs":[]}]}